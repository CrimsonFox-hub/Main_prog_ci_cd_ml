"""
–ü—Ä–æ—Å—Ç–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
"""
import subprocess
import sys
import os
from pathlib import Path

def run_command(command, description):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    print(f"\n{'='*60}")
    print(f"{description}")
    print(f"{'='*60}")
    print(f"–ö–æ–º–∞–Ω–¥–∞: {command}")
    
    try:
        result = subprocess.run(command, shell=True, check=True, 
                              capture_output=True, text=True)
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ")
        if result.stdout:
            print(f"–í—ã–≤–æ–¥:\n{result.stdout}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ (–∫–æ–¥ {e.returncode}):")
        if e.stdout:
            print(f"–í—ã–≤–æ–¥:\n{e.stdout}")
        if e.stderr:
            print(f"–û—à–∏–±–∫–∞:\n{e.stderr}")
        return False

def main():
    print("üöÄ –ó–∞–ø—É—Å–∫ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞")
    print("="*60)
    
    # 1. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    print("\n1. –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π...")
    directories = [
        "data/raw",
        "data/processed",
        "models/trained",
        "logs",
        "reports",
        "configs"
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        print(f"   –°–æ–∑–¥–∞–Ω–∞: {directory}")
    
    # 2. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\n2. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    
    np.random.seed(42)
    n_samples = 1000
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    data = {
        'age': np.random.randint(18, 70, n_samples),
        'income': np.random.randint(20000, 150000, n_samples),
        'credit_amount': np.random.randint(1000, 50000, n_samples),
        'loan_duration': np.random.randint(6, 60, n_samples),
        'payment_to_income': np.random.uniform(0.1, 0.5, n_samples),
        'existing_credits': np.random.randint(0, 5, n_samples),
        'dependents': np.random.randint(0, 5, n_samples),
        'residence_since': np.random.randint(0, 20, n_samples),
        'installment_rate': np.random.uniform(1.0, 4.0, n_samples),
        'target': np.random.binomial(1, 0.3, n_samples)
    }
    
    df = pd.DataFrame(data)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    train_df, test_df = train_test_split(
        df, test_size=0.2, random_state=42, stratify=df['target']
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    train_df.to_csv("data/processed/train.csv", index=False)
    test_df.to_csv("data/processed/test.csv", index=False)
    
    print(f"   –°–æ–∑–¥–∞–Ω–æ {len(train_df)} train –∏ {len(test_df)} test –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"   –î–µ—Ñ–æ–ª—Ç–æ–≤ –≤ train: {train_df['target'].mean():.2%}")
    print(f"   –î–µ—Ñ–æ–ª—Ç–æ–≤ –≤ test: {test_df['target'].mean():.2%}")
    
    # 3. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    print("\n3. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
    config_content = """project:
  name: "credit-scoring-test"
  version: "1.0.0"

data:
  train_path: "data/processed/train.csv"
  target_column: "target"
  
model:
  name: "SimpleCreditNN"
  hidden_layers: [64, 32]
  dropout_rate: 0.3
  
  paths:
    best_model: "models/trained/best_model.pth"
    final_model: "models/trained/final_model.pth"
    scaler: "models/trained/scaler.pkl"
    metrics: "models/trained/training_metrics.json"

training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.001
"""
    
    with open("configs/simple_test.yaml", "w", encoding="utf-8") as f:
        f.write(config_content)
    print("   –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: configs/simple_test.yaml")
    
    # 4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n4. –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")
    
    # –°–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è
    train_script = """
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from pathlib import Path
import yaml
import joblib
from sklearn.preprocessing import StandardScaler

# –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å
class SimpleModel(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.net(x)

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
train_df = pd.read_csv("data/processed/train.csv")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
X = train_df.drop(columns=['target']).values.astype(np.float32)
y = train_df['target'].values.astype(np.float32)

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–∞
Path("models/trained").mkdir(parents=True, exist_ok=True)
joblib.dump(scaler, "models/trained/scaler.pkl")

# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä—ã
X_tensor = torch.FloatTensor(X_scaled)
y_tensor = torch.FloatTensor(y).unsqueeze(1)

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = SimpleModel(X.shape[1])
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# –û–±—É—á–µ–Ω–∏–µ
print("–û–±—É—á–µ–Ω–∏–µ...")
for epoch in range(10):
    model.train()
    optimizer.zero_grad()
    
    outputs = model(X_tensor)
    loss = criterion(outputs, y_tensor)
    loss.backward()
    optimizer.step()
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ —Ç–æ—á–Ω–æ—Å—Ç—å
    with torch.no_grad():
        predictions = (outputs > 0.5).float()
        accuracy = (predictions == y_tensor).float().mean()
    
    if epoch % 2 == 0:
        print(f"Epoch {epoch}: Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
torch.save(model.state_dict(), "models/trained/model.pth")
print("–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/trained/model.pth")

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
print("\\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...")
test_df = pd.read_csv("data/processed/test.csv")
X_test = test_df.drop(columns=['target']).values.astype(np.float32)
y_test = test_df['target'].values.astype(np.float32)
X_test_scaled = scaler.transform(X_test)

model.eval()
with torch.no_grad():
    X_test_tensor = torch.FloatTensor(X_test_scaled)
    predictions = model(X_test_tensor)
    predictions_bin = (predictions > 0.5).float()
    accuracy = (predictions_bin == torch.FloatTensor(y_test).unsqueeze(1)).float().mean()
    
print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {accuracy.item():.4f}")
print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
"""
    
    with open("train_simple.py", "w", encoding="utf-8") as f:
        f.write(train_script)
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    result = run_command("python train_simple.py", "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")
    
    # 5. –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    if Path("train_simple.py").exists():
        Path("train_simple.py").unlink()
    
    print("\n" + "="*60)
    if result:
        print("üéâ –¢–µ—Å—Ç–æ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω!")
        print("–°–æ–∑–¥–∞–Ω—ã:")
        print("  - data/processed/train.csv")
        print("  - data/processed/test.csv")
        print("  - models/trained/model.pth")
        print("  - models/trained/scaler.pkl")
    else:
        print("‚ö†Ô∏è –í –ø—Ä–æ—Ü–µ—Å—Å–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤–æ–∑–Ω–∏–∫–ª–∏ –æ—à–∏–±–∫–∏")
    
    print("="*60)

if __name__ == "__main__":
    main()