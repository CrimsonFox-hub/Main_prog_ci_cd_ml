"""
Airflow DAG –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
–≠—Ç–∞–ø 7: –ü–∞–π–ø–ª–∞–π–Ω –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
from airflow.providers.cncf.kubernetes.secret import Secret
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from airflow.models import Variable, Connection
from airflow.utils.trigger_rule import TriggerRule
from kubernetes.client import models as k8s_models
import pendulum
import json
import yaml
from pathlib import Path

# –õ–æ–∫–∞–ª—å–Ω—ã–π —á–∞—Å–æ–≤–æ–π –ø–æ—è—Å
local_tz = pendulum.timezone("Europe/Moscow")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
default_args = {
    'owner': 'ml-engineering',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1, tzinfo=local_tz),
    'email': ['ml-team@your-bank.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'max_active_runs': 1,
    'catchup': False,
    'execution_timeout': timedelta(hours=6)
}

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
with open('/opt/airflow/configs/ml_pipeline_config.yaml', 'r') as f:
    config = yaml.safe_load(f)

def check_drift(**context):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—Ä–∏—Ñ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Ü–µ–ø—Ç–∞"""
    import requests
    import pandas as pd
    from datetime import datetime, timedelta
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    monitoring_api = Variable.get("monitoring_api_endpoint")
    
    try:
        response = requests.get(
            f"{monitoring_api}/api/v1/drift/latest",
            headers={'Authorization': f"Bearer {Variable.get('monitoring_api_token')}"},
            timeout=30
        )
        
        if response.status_code == 200:
            drift_data = response.json()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π –¥—Ä–∏—Ñ—Ç–∞
            data_drift_detected = drift_data.get('data_drift_detected', False)
            concept_drift_detected = drift_data.get('concept_drift_detected', False)
            performance_decay = drift_data.get('significant_performance_decay', False)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ)
            last_update = datetime.fromisoformat(drift_data.get('timestamp', '2000-01-01'))
            is_recent = (datetime.now() - last_update) < timedelta(days=2)
            
            should_retrain = (
                (data_drift_detected and drift_data.get('data_drift_score', 0) > 0.3) or
                (concept_drift_detected and drift_data.get('concept_drift_score', 0) > 0.25) or
                performance_decay or
                (not is_recent)  # –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —É—Å—Ç–∞—Ä–µ–ª–∏
            )
            
            # Push –≤ XCom
            context['ti'].xcom_push(key='should_retrain', value=should_retrain)
            context['ti'].xcom_push(key='drift_data', value=drift_data)
            
            return {
                'should_retrain': should_retrain,
                'data_drift_score': drift_data.get('data_drift_score', 0),
                'concept_drift_score': drift_data.get('concept_drift_score', 0),
                'reason': 'drift_detected' if should_retrain else 'no_drift'
            }
            
    except Exception as e:
        # –ï—Å–ª–∏ API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤—Å–µ —Ä–∞–≤–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
        print(f"Monitoring API error: {str(e)}. Proceeding with scheduled retraining.")
        
        context['ti'].xcom_push(key='should_retrain', value=True)
        context['ti'].xcom_push(key='drift_data', value={'error': str(e)})
        
        return {
            'should_retrain': True,
            'reason': 'monitoring_unavailable'
        }

def check_data_availability(**context):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    import boto3
    from datetime import datetime, timedelta
    
    s3_client = boto3.client(
        's3',
        endpoint_url=Variable.get('s3_endpoint'),
        aws_access_key_id=Variable.get('s3_access_key'),
        aws_secret_access_key=Variable.get('s3_secret_key')
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    bucket_name = Variable.get('data_bucket_name')
    prefix = 'raw/credit_data/'
    
    try:
        response = s3_client.list_objects_v2(
            Bucket=bucket_name,
            Prefix=prefix,
            MaxKeys=10
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤–µ–∂–µ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
        latest_data = None
        if 'Contents' in response:
            latest_data = max(response['Contents'], key=lambda x: x['LastModified'])
            data_age = datetime.now(latest_data['LastModified'].tzinfo) - latest_data['LastModified']
            
            has_new_data = data_age < timedelta(days=7)
            
            context['ti'].xcom_push(key='has_new_data', value=has_new_data)
            context['ti'].xcom_push(key='latest_data_age_days', value=data_age.days)
            
            return {
                'has_new_data': has_new_data,
                'latest_data_age_days': data_age.days,
                'latest_data_key': latest_data['Key']
            }
        else:
            context['ti'].xcom_push(key='has_new_data', value=False)
            return {'has_new_data': False, 'error': 'no_data_found'}
            
    except Exception as e:
        print(f"Error checking data availability: {str(e)}")
        context['ti'].xcom_push(key='has_new_data', value=False)
        return {'has_new_data': False, 'error': str(e)}

def decide_retraining(**context):
    """–ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–∏"""
    should_retrain = context['ti'].xcom_pull(task_ids='check_drift', key='should_retrain')
    has_new_data = context['ti'].xcom_pull(task_ids='check_data_availability', key='has_new_data')
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏—á–∏–Ω—ã –∏–∑ check_drift
    drift_result = context['ti'].xcom_pull(task_ids='check_drift')
    reason = drift_result.get('reason', 'scheduled') if isinstance(drift_result, dict) else 'scheduled'
    
    # –£—Å–ª–æ–≤–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    conditions_met = should_retrain or has_new_data
    
    if not conditions_met:
        print("Retraining conditions not met. Skipping...")
        context['ti'].xcom_push(key='retrain_decision', value=False)
        return {'decision': 'skip', 'reason': 'conditions_not_met'}
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
    priority = 'high' if should_retrain else 'medium'
    
    # Push —Ä–µ—à–µ–Ω–∏—è –≤ XCom
    context['ti'].xcom_push(key='retrain_decision', value=True)
    context['ti'].xcom_push(key='retrain_priority', value=priority)
    context['ti'].xcom_push(key='retrain_reason', value=reason)
    
    return {
        'decision': 'proceed',
        'priority': priority,
        'reason': reason,
        'conditions': {
            'should_retrain': should_retrain,
            'has_new_data': has_new_data
        }
    }

def send_slack_notification(**context):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ Slack"""
    decision = context['ti'].xcom_pull(task_ids='decide_retraining')
    
    if decision.get('decision') == 'skip':
        message = "üü° *Scheduled Model Retraining Skipped*\n"
        message += f"Reason: {decision.get('reason', 'Conditions not met')}\n"
        color = "warning"
    else:
        message = "üü¢ *Model Retraining Started*\n"
        message += f"Priority: {decision.get('priority', 'medium')}\n"
        message += f"Reason: {decision.get('reason', 'scheduled')}\n"
        message += f"DAG Run: {context['run_id']}\n"
        color = "good"
    
    slack_webhook_token = Variable.get("slack_webhook_token")
    
    slack_task = SlackWebhookOperator(
        task_id='send_slack_notification',
        http_conn_id='slack_webhook',
        message=message,
        username='Airflow ML Pipeline',
        icon_emoji=':robot_face:',
        dag=dag
    )
    
    return slack_task.execute(context)

# –°–µ–∫—Ä–µ—Ç—ã Kubernetes
secrets = [
    Secret(
        deploy_type='env',
        deploy_target='MLFLOW_TRACKING_URI',
        secret='mlflow-secrets',
        key='tracking-uri'
    ),
    Secret(
        deploy_type='env',
        deploy_target='DVC_REMOTE_URL',
        secret='dvc-secrets',
        key='remote-url'
    ),
    Secret(
        deploy_type='env',
        deploy_target='S3_ACCESS_KEY',
        secret='s3-secrets',
        key='access-key'
    ),
    Secret(
        deploy_type='env',
        deploy_target='S3_SECRET_KEY',
        secret='s3-secrets',
        key='secret-key'
    )
]

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
resources = k8s_models.V1ResourceRequirements(
    requests={
        'memory': '8Gi',
        'cpu': '4',
        'nvidia.com/gpu': '1' if config['training']['use_gpu'] else None
    },
    limits={
        'memory': '16Gi',
        'cpu': '8',
        'nvidia.com/gpu': '1' if config['training']['use_gpu'] else None
    }
)

# –°–æ–∑–¥–∞–Ω–∏–µ DAG
with DAG(
    dag_id='credit_scoring_retraining',
    default_args=default_args,
    description='Automated retraining pipeline for credit scoring model',
    schedule_interval=timedelta(days=7),  # –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ
    catchup=False,
    tags=['mlops', 'retraining', 'production'],
    concurrency=1,
    max_active_runs=1,
    on_success_callback=None,
    on_failure_callback=None,
) as dag:
    
    # –°—Ç–∞—Ä—Ç–æ–≤–∞—è –∑–∞–¥–∞—á–∞
    start = DummyOperator(
        task_id='start',
        dag=dag
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—Ä–∏—Ñ—Ç–∞
    check_drift_task = PythonOperator(
        task_id='check_drift',
        python_callable=check_drift,
        provide_context=True,
        execution_timeout=timedelta(minutes=10),
        dag=dag
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
    check_data_task = PythonOperator(
        task_id='check_data_availability',
        python_callable=check_data_availability,
        provide_context=True,
        execution_timeout=timedelta(minutes=5),
        dag=dag
    )
    
    # –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è
    decide_task = PythonOperator(
        task_id='decide_retraining',
        python_callable=decide_retraining,
        provide_context=True,
        dag=dag
    )
    
    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –≤ Slack
    notify_start = PythonOperator(
        task_id='notify_start',
        python_callable=send_slack_notification,
        provide_context=True,
        dag=dag
    )
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    prepare_data = KubernetesPodOperator(
        task_id='prepare_training_data',
        namespace='ml-training',
        image=f"{Variable.get('image_registry')}/data-preparation:latest",
        cmds=['python', '-m', 'src.data_preparation.pipeline'],
        arguments=[
            '--input-path', '/data/raw',
            '--output-path', '/data/processed',
            '--config', '/app/configs/training_config.yaml'
        ],
        secrets=secrets,
        name='prepare-data-pod',
        is_delete_operator_pod=True,
        get_logs=True,
        resources=resources,
        image_pull_policy='Always',
        env_vars={
            'EXECUTION_DATE': '{{ ds }}',
            'DAG_RUN_ID': '{{ run_id }}'
        },
        volumes=[
            k8s_models.V1Volume(
                name='data-volume',
                persistent_volume_claim=k8s_models.V1PersistentVolumeClaimVolumeSource(
                    claim_name='data-pvc'
                )
            )
        ],
        volume_mounts=[
            k8s_models.V1VolumeMount(
                name='data-volume',
                mount_path='/data'
            )
        ],
        dag=dag
    )
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    train_model = KubernetesPodOperator(
        task_id='train_model',
        namespace='ml-training',
        image=f"{Variable.get('image_registry')}/model-training:latest",
        cmds=['python', '-m', 'src.ml_pipeline.training.train_model'],
        arguments=[
            '--config', '/app/configs/training_config.yaml',
            '--data-path', '/data/processed/train.csv',
            '--output-path', '/models',
            '--experiment-name', 'credit_scoring_retraining_{{ ds_nodash }}'
        ],
        secrets=secrets,
        name='train-model-pod',
        is_delete_operator_pod=True,
        get_logs=True,
        resources=resources,
        image_pull_policy='Always',
        env_vars={
            'MLFLOW_EXPERIMENT_NAME': 'credit_scoring_retraining',
            'MLFLOW_RUN_NAME': 'run_{{ ds_nodash }}_{{ ts_nodash }}'
        },
        volumes=[
            k8s_models.V1Volume(
                name='data-volume',
                persistent_volume_claim=k8s_models.V1PersistentVolumeClaimVolumeSource(
                    claim_name='data-pvc'
                )
            ),
            k8s_models.V1Volume(
                name='models-volume',
                persistent_volume_claim=k8s_models.V1PersistentVolumeClaimVolumeSource(
                    claim_name='models-pvc'
                )
            )
        ],
        volume_mounts=[
            k8s_models.V1VolumeMount(
                name='data-volume',
                mount_path='/data'
            ),
            k8s_models.V1VolumeMount(
                name='models-volume',
                mount_path='/models'
            )
        ],
        dag=dag
    )
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ ONNX
    convert_model = KubernetesPodOperator(
        task_id='convert_to_onnx',
        namespace='ml-training',
        image=f"{Variable.get('image_registry')}/model-conversion:latest",
        cmds=['python', '-m', 'src.ml_pipeline.training.onnx_conversion'],
        arguments=[
            '--model-path', '/models/best_model.pth',
            '--output-path', '/models/converted',
            '--input-shape', '1,20'
        ],
        secrets=secrets,
        name='convert-model-pod',
        is_delete_operator_pod=True,
        get_logs=True,
        dag=dag
    )
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    validate_model = KubernetesPodOperator(
        task_id='validate_model',
        namespace='ml-training',
        image=f"{Variable.get('image_registry')}/model-validation:latest",
        cmds=['python', '-m', 'src.ml_pipeline.validation.validate_model'],
        arguments=[
            '--model-path', '/models/converted/model.onnx',
            '--test-data', '/data/processed/test.csv',
            '--metrics-output', '/reports/validation_metrics.json'
        ],
        secrets=secrets,
        name='validate-model-pod',
        is_delete_operator_pod=True,
        get_logs=True,
        dag=dag
    )
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª—å—é
    compare_models = KubernetesPodOperator(
        task_id='compare_with_current',
        namespace='ml-training',
        image=f"{Variable.get('image_registry')}/model-comparison:latest",
        cmds=['python', '-m', 'src.ml_pipeline.validation.model_comparison'],
        arguments=[
            '--new-model', '/models/converted/model.onnx',
            '--current-model', '/models/current/model.onnx',
            '--test-data', '/data/processed/test.csv',
            '--output', '/reports/comparison_report.json'
        ],
        secrets=secrets,
        name='compare-models-pod',
        is_delete_operator_pod=True,
        get_logs=True,
        dag=dag
    )
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ MLflow
    register_model = KubernetesPodOperator(
        task_id='register_model',
        namespace='ml-training',
        image=f"{Variable.get('image_registry')}/model-registration:latest",
        cmds=['python', '-m', 'src.ml_pipeline.registration.register_model'],
        arguments=[
            '--model-path', '/models/converted/model.onnx',
            '--run-id', '{{ task_instance.xcom_pull(task_ids="train_model")["run_id"] }}',
            '--stage', 'Staging',
            '--description', 'Automated retraining {{ ds }}'
        ],
        secrets=secrets,
        name='register-model-pod',
        is_delete_operator_pod=True,
        get_logs=True,
        dag=dag
    )
    
    # A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    ab_testing = KubernetesPodOperator(
        task_id='ab_testing',
        namespace='ml-production',
        image=f"{Variable.get('image_registry')}/ab-testing:latest",
        cmds=['python', '-m', 'src.ml_pipeline.testing.ab_test'],
        arguments=[
            '--model-a', 'current',
            '--model-b', 'staging',
            '--traffic-percent', '10',
            '--duration-hours', '24'
        ],
        secrets=secrets,
        name='ab-testing-pod',
        is_delete_operator_pod=True,
        get_logs=True,
        dag=dag
    )
    
    # –ü—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ Production
    promote_model = KubernetesPodOperator(
        task_id='promote_to_production',
        namespace='ml-production',
        image=f"{Variable.get('image_registry')}/model-promotion:latest",
        cmds=['python', '-m', 'src.ml_pipeline.deployment.promote_model'],
        arguments=[
            '--model-version', '{{ task_instance.xcom_pull(task_ids="register_model")["model_version"] }}',
            '--validation-report', '/reports/comparison_report.json',
            '--strategy', 'canary',
            '--traffic-percent', '50'
        ],
        secrets=secrets,
        name='promote-model-pod',
        is_delete_operator_pod=True,
        get_logs=True,
        dag=dag
    )
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    update_monitoring = KubernetesPodOperator(
        task_id='update_monitoring',
        namespace='ml-production',
        image=f"{Variable.get('image_registry')}/monitoring-update:latest",
        cmds=['python', '-m', 'src.ml_pipeline.monitoring.update_reference'],
        arguments=[
            '--new-reference', '/data/processed/train.csv',
            '--model-version', '{{ task_instance.xcom_pull(task_ids="register_model")["model_version"] }}'
        ],
        secrets=secrets,
        name='update-monitoring-pod',
        is_delete_operator_pod=True,
        get_logs=True,
        dag=dag
    )
    
    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± —É—Å–ø–µ—à–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
    notify_success = SlackWebhookOperator(
        task_id='notify_success',
        http_conn_id='slack_webhook',
        message="""
‚úÖ *Model Retraining Completed Successfully*
‚Ä¢ New model version: {{ task_instance.xcom_pull(task_ids="register_model")["model_version"] }}
‚Ä¢ Performance improvement: {{ task_instance.xcom_pull(task_ids="compare_with_current")["improvement"] }}%
‚Ä¢ Deployed with canary strategy (50% traffic)
‚Ä¢ Monitoring reference data updated
        """,
        username='Airflow ML Pipeline',
        icon_emoji=':rocket:',
        trigger_rule=TriggerRule.ALL_SUCCESS,
        dag=dag
    )
    
    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –Ω–µ—É–¥–∞—á–µ
    notify_failure = SlackWebhookOperator(
        task_id='notify_failure',
        http_conn_id='slack_webhook',
        message="""
‚ùå *Model Retraining Failed*
‚Ä¢ DAG Run: {{ run_id }}
‚Ä¢ Failed Task: {{ task_instance.task_id }}
‚Ä¢ Error: {{ task_instance.state }}
‚Ä¢ Check Airflow logs for details
        """,
        username='Airflow ML Pipeline',
        icon_emoji=':x:',
        trigger_rule=TriggerRule.ONE_FAILED,
        dag=dag
    )
    
    # –ó–∞–¥–∞—á–∞ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    end_success = DummyOperator(
        task_id='end_success',
        trigger_rule=TriggerRule.ALL_SUCCESS,
        dag=dag
    )
    
    # –ó–∞–¥–∞—á–∞ –Ω–µ—É–¥–∞—á–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    end_failure = DummyOperator(
        task_id='end_failure',
        trigger_rule=TriggerRule.ONE_FAILED,
        dag=dag
    )
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∑–∞–¥–∞—á
    start >> [check_drift_task, check_data_task] >> decide_task >> notify_start
    
    # –ï—Å–ª–∏ –ø—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–∏
    notify_start >> prepare_data >> train_model >> convert_model >> validate_model
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    validate_model >> [compare_models, register_model]
    
    # –ü–æ—Å–ª–µ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    [compare_models, register_model] >> ab_testing >> promote_model >> update_monitoring
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
    update_monitoring >> [notify_success, end_success]
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
    [train_model, convert_model, validate_model, compare_models, 
     register_model, ab_testing, promote_model, update_monitoring] >> notify_failure >> end_failure

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å XCom
def get_model_version(**context):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏ –∏–∑ XCom"""
    model_version = context['ti'].xcom_pull(task_ids='register_model', key='model_version')
    return model_version or 'unknown'

def get_validation_results(**context):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    validation_results = context['ti'].xcom_pull(task_ids='validate_model')
    return validation_results or {}

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è DAG
dag.doc_md = __doc__
dag.owner_links = {"ml-engineering": "mailto:ml-team@your-bank.com"}

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
dag.tags = ['mlops', 'retraining', 'credit-scoring', 'production']

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ SLA
dag.sla = timedelta(hours=8)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
dag.params = {
    'enable_ab_testing': True,
    'canary_traffic_percent': 50,
    'validation_threshold': 0.02,
    'max_training_time_hours': 4
}