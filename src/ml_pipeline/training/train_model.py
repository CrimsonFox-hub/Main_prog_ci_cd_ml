"""
–ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å ONNX —ç–∫—Å–ø–æ—Ä—Ç–æ–º
–≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –∫ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–π —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import joblib
import onnx
import onnxruntime as ort
from pathlib import Path
import json
import time
from datetime import datetime
import yaml
import warnings
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score
warnings.filterwarnings('ignore')

# ==================== 1. –ù–ï–ô–†–û–ù–ù–ê–Ø –°–ï–¢–¨ ====================
class CreditScoringNN(nn.Module):
    """–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞"""
    def __init__(self, input_size, hidden_layers=[128, 64, 32], dropout_rate=0.3):
        super(CreditScoringNN, self).__init__()
        
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_layers:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.BatchNorm1d(hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
            prev_size = hidden_size
        
        self.hidden_layers = nn.Sequential(*layers)
        self.output_layer = nn.Sequential(
            nn.Linear(prev_size, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        x = self.hidden_layers(x)
        return self.output_layer(x)

# ==================== 2. –î–ê–¢–ê–°–ï–¢ ====================
class CreditDataset(Dataset):
    def __init__(self, features, labels=None):
        self.features = torch.FloatTensor(features)
        self.has_labels = labels is not None
        if self.has_labels:
            self.labels = torch.FloatTensor(labels)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        if self.has_labels:
            return self.features[idx], self.labels[idx]
        return self.features[idx]

# ==================== 3. –û–ë–£–ß–ï–ù–ò–ï ====================
def train_neural_network(config):
    """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
    print("=" * 60)
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_df = pd.read_csv(config['data']['train_path'])
    test_df = pd.read_csv(config['data']['test_path'])
    
    target_col = config['data']['target_column']
    
    X_train = train_df.drop(columns=[target_col]).values.astype(np.float32)
    y_train = train_df[target_col].values.astype(np.float32)
    X_test = test_df.drop(columns=[target_col]).values.astype(np.float32)
    y_test = test_df[target_col].values.astype(np.float32)
    
    print(f"   Train: {X_train.shape}, Test: {X_test.shape}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = CreditDataset(X_train, y_train)
    test_dataset = CreditDataset(X_test, y_test)
    
    batch_size = config['training']['batch_size']
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    input_size = X_train.shape[1]
    model = CreditScoringNN(
        input_size=input_size,
        hidden_layers=config['model']['hidden_layers'],
        dropout_rate=config['model']['dropout_rate']
    )
    
    # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    criterion = nn.BCELoss()
    optimizer = optim.Adam(
        model.parameters(),
        lr=config['training']['learning_rate'],
        weight_decay=config['training']['weight_decay']
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    epochs = config['training']['epochs']
    print(f"\n‚öôÔ∏è  –ù–∞—Å—Ç—Ä–æ–π–∫–∏:")
    print(f"   –≠–ø–æ—Ö–∏: {epochs}, Batch: {batch_size}, LR: {config['training']['learning_rate']}")
    print(f"   –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {config['model']['hidden_layers']}")
    
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    
    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0.0
        
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs.squeeze(), batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # Validation
        model.eval()
        val_loss = 0.0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                outputs = model(batch_x)
                loss = criterion(outputs.squeeze(), batch_y)
                val_loss += loss.item()
                
                all_preds.extend(outputs.squeeze().numpy())
                all_labels.extend(batch_y.numpy())
        
        avg_val_loss = val_loss / len(test_loader)
        val_losses.append(avg_val_loss)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        predictions = (np.array(all_preds) > 0.5).astype(int)
        accuracy = accuracy_score(all_labels, predictions)
        f1 = f1_score(all_labels, predictions)
        
        if epoch % 10 == 0 or epoch == epochs - 1:
            print(f"   Epoch {epoch:3d}: "
                  f"Train Loss: {avg_train_loss:.4f}, "
                  f"Val Loss: {avg_val_loss:.4f}, "
                  f"Acc: {accuracy:.4f}, F1: {f1:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': best_val_loss,
                'accuracy': accuracy,
                'f1_score': f1,
                'input_size': input_size
            }, config['model_paths']['best_model'])
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    torch.save(model.state_dict(), config['model_paths']['final_model'])
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    model.eval()
    test_preds = []
    with torch.no_grad():
        for batch_x, _ in test_loader:
            outputs = model(batch_x)
            test_preds.extend(outputs.squeeze().numpy())
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–µ
    test_predictions = (np.array(test_preds) > 0.5).astype(int)
    test_accuracy = accuracy_score(y_test, test_predictions)
    test_f1 = f1_score(y_test, test_predictions)
    test_roc_auc = roc_auc_score(y_test, test_preds)
    
    print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   –õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è loss: {best_val_loss:.4f}")
    print(f"   –¢–µ—Å—Ç Accuracy: {test_accuracy:.4f}")
    print(f"   –¢–µ—Å—Ç F1: {test_f1:.4f}")
    print(f"   –¢–µ—Å—Ç ROC-AUC: {test_roc_auc:.4f}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    metrics = {
        'best_val_loss': float(best_val_loss),
        'test_accuracy': float(test_accuracy),
        'test_f1_score': float(test_f1),
        'test_roc_auc': float(test_roc_auc),
        'train_losses': [float(x) for x in train_losses],
        'val_losses': [float(x) for x in val_losses],
        'input_size': input_size,
        'training_time': datetime.now().isoformat(),
        'model_architecture': str(config['model']['hidden_layers']),
        'training_params': {
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': config['training']['learning_rate']
        }
    }
    
    with open(config['model_paths']['metrics'], 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {config['model_paths']['metrics']}")
    
    return model, input_size

# ==================== 4. ONNX –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø ====================
def convert_to_onnx(model, input_size, config):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ ONNX —Ñ–æ—Ä–º–∞—Ç"""
    print("\n" + "=" * 60)
    print("üîÑ –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –í ONNX")
    print("=" * 60)
    
    model.eval()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π batch size
    batch_size = 1  # –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
    dummy_input = torch.randn(batch_size, input_size, requires_grad=True)
    
    onnx_path = config['model_paths']['onnx']
    
    print(f"   –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ –≤ ONNX: {onnx_path}")
    
    # –≠–∫—Å–ø–æ—Ä—Ç —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –æ—Å—è–º–∏
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=14,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},  # –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ—Å—å batch
            'output': {0: 'batch_size'}
        },
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata={'model_type': 'credit_scoring', 'version': '1.0.0'}
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ ONNX –º–æ–¥–µ–ª–∏
    try:
        onnx_model = onnx.load(onnx_path)
        onnx.checker.check_model(onnx_model)
        print("   ‚úÖ ONNX –º–æ–¥–µ–ª—å –≤–∞–ª–∏–¥–Ω–∞")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å ONNX Runtime
        ort_session = ort.InferenceSession(onnx_path)
        print(f"   ‚úÖ ONNX Runtime —Å–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞")
        print(f"   üè∑Ô∏è  –í—Ö–æ–¥–Ω—ã–µ –∏–º–µ–Ω–∞: {[input.name for input in ort_session.get_inputs()]}")
        print(f"   üè∑Ô∏è  –í—ã—Ö–æ–¥–Ω—ã–µ –∏–º–µ–Ω–∞: {[output.name for output in ort_session.get_outputs()]}")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ ONNX: {str(e)}")
        print("   –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ...")
    
    return onnx_path

# ==================== 5. –ë–ï–ù–ß–ú–ê–†–ö–ò–ù–ì ====================
def benchmark_models(config, input_size):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    print("\n" + "=" * 60)
    print("‚ö° –ë–ï–ù–ß–ú–ê–†–ö–ò–ù–ì –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
    print("=" * 60)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    dummy_input_np = np.random.randn(1000, input_size).astype(np.float32)
    dummy_input_torch = torch.FloatTensor(dummy_input_np)
    
    results = []
    
    # 1. PyTorch –º–æ–¥–µ–ª—å
    print("\n1. PyTorch –º–æ–¥–µ–ª—å (CPU):")
    model = CreditScoringNN(input_size)
    model.load_state_dict(torch.load(config['model_paths']['final_model']))
    model.eval()
    
    start = time.time()
    with torch.no_grad():
        for i in range(0, 1000, 100):
            batch = dummy_input_torch[i:i+100]
            _ = model(batch)
    torch_time = time.time() - start
    
    results.append({
        'model': 'PyTorch',
        'samples_per_sec': 1000 / torch_time,
        'latency_ms': (torch_time / 1000) * 1000
    })
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {1000/torch_time:.2f} samples/sec")
    print(f"   –ó–∞–¥–µ—Ä–∂–∫–∞: {(torch_time/1000)*1000:.2f} ms/sample")
    
    # 2. ONNX –º–æ–¥–µ–ª—å
    print("\n2. ONNX –º–æ–¥–µ–ª—å (CPU):")
    ort_session = ort.InferenceSession(config['model_paths']['onnx'])
    
    start = time.time()
    for i in range(0, 1000, 100):
        batch = dummy_input_np[i:i+100]
        _ = ort_session.run(None, {'input': batch})
    onnx_time = time.time() - start
    
    results.append({
        'model': 'ONNX',
        'samples_per_sec': 1000 / onnx_time,
        'latency_ms': (onnx_time / 1000) * 1000,
        'speedup_vs_pytorch': torch_time / onnx_time
    })
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {1000/onnx_time:.2f} samples/sec")
    print(f"   –ó–∞–¥–µ—Ä–∂–∫–∞: {(onnx_time/1000)*1000:.2f} ms/sample")
    print(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {torch_time/onnx_time:.2f}x")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    benchmark_path = 'reports/benchmark_results.json'
    Path('reports').mkdir(exist_ok=True)
    
    with open(benchmark_path, 'w') as f:
        json.dump({
            'benchmark_results': results,
            'timestamp': datetime.now().isoformat(),
            'hardware': {
                'cpu_cores': torch.get_num_threads(),
                'device': 'CPU'
            },
            'summary': {
                'best_performance': max(results, key=lambda x: x['samples_per_sec'])['model'],
                'pytorch_to_onnx_speedup': torch_time / onnx_time if 'onnx_time' in locals() else None
            }
        }, f, indent=2)
    
    print(f"\nüìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {benchmark_path}")
    
    return results

# ==================== 6. –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø ====================
def optimize_model(config, input_size):
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (Quantization/Pruning)"""
    print("\n" + "=" * 60)
    print("üìâ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model = CreditScoringNN(input_size)
    model.load_state_dict(torch.load(config['model_paths']['final_model']))
    model.eval()
    
    # 1. Dynamic Quantization (8-bit)
    print("\n1. Dynamic Quantization (INT8):")
    quantized_model = torch.quantization.quantize_dynamic(
        model, {nn.Linear}, dtype=torch.qint8
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    quantized_path = config['model_paths']['final_model'].replace('.pth', '_quantized.pth')
    torch.jit.save(torch.jit.script(quantized_model), quantized_path)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤
    original_size = Path(config['model_paths']['final_model']).stat().st_size / 1024
    quantized_size = Path(quantized_path).stat().st_size / 1024
    
    print(f"   –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {original_size:.1f} KB")
    print(f"   –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {quantized_size:.1f} KB")
    print(f"   –°–∂–∞—Ç–∏–µ: {original_size/quantized_size:.1f}x")
    
    # 2. Pruning (–£–¥–∞–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤)
    print("\n2. Pruning (–£–¥–∞–ª–µ–Ω–∏–µ 20% –≤–µ—Å–æ–≤):")
    parameters_to_prune = []
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            parameters_to_prune.append((module, 'weight'))
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º pruning
    from torch.nn.utils import prune
    for module, param_name in parameters_to_prune:
        prune.l1_unstructured(module, name=param_name, amount=0.2)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ pruned –º–æ–¥–µ–ª–∏
    pruned_path = config['model_paths']['final_model'].replace('.pth', '_pruned.pth')
    torch.save(model.state_dict(), pruned_path)
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ pruned –º–æ–¥–µ–ª–∏
    model.eval()
    dummy_input = torch.randn(1, input_size)
    with torch.no_grad():
        start = time.time()
        for _ in range(1000):
            _ = model(dummy_input)
        pruned_time = time.time() - start
    
    print(f"   –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (pruned): {pruned_time/1000*1000:.2f} ms/sample")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    optimization_report = {
        'quantization': {
            'original_size_kb': original_size,
            'quantized_size_kb': quantized_size,
            'compression_ratio': original_size / quantized_size
        },
        'pruning': {
            'pruned_parameters_percentage': 20,
            'inference_time_ms': pruned_time / 1000 * 1000
        },
        'timestamp': datetime.now().isoformat()
    }
    
    optimization_path = 'reports/optimization_report.json'
    with open(optimization_path, 'w') as f:
        json.dump(optimization_report, f, indent=2)
    
    print(f"\nüìä –û—Ç—á–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {optimization_path}")
    
    return quantized_path, pruned_path

# ==================== 7. –ì–õ–ê–í–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù ====================
def main():
    """–ì–ª–∞–≤–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω"""
    print("=" * 60)
    print("üéØ –ü–†–û–ú–´–®–õ–ï–ù–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 60)
    
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', default='configs/training_config.yaml')
    parser.add_argument('--skip-optimization', action='store_true')
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    Path('models/trained').mkdir(parents=True, exist_ok=True)
    Path('models/processed').mkdir(parents=True, exist_ok=True)
    Path('reports').mkdir(parents=True, exist_ok=True)
    
    # 1. –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
    model, input_size = train_neural_network(config)
    
    # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ ONNX
    onnx_path = convert_to_onnx(model, input_size, config)
    
    # 3. –ë–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥
    benchmark_results = benchmark_models(config, input_size)
    
    # 4. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –Ω–µ –ø—Ä–æ–ø—É—â–µ–Ω–∞)
    if not args.skip_optimization:
        quantized_path, pruned_path = optimize_model(config, input_size)
    
    # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    print("\n" + "=" * 60)
    print("üìã –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç—Ä–∏–∫
    with open(config['model_paths']['metrics'], 'r') as f:
        metrics = json.load(f)
    
    print(f"\nüìà –ú–ï–¢–†–ò–ö–ò –ú–û–î–ï–õ–ò:")
    print(f"   ROC-AUC: {metrics['test_roc_auc']:.4f}")
    print(f"   Accuracy: {metrics['test_accuracy']:.4f}")
    print(f"   F1-Score: {metrics['test_f1_score']:.4f}")
    
    print(f"\nüíæ –§–ê–ô–õ–´ –ú–û–î–ï–õ–ò:")
    print(f"   PyTorch: {config['model_paths']['final_model']}")
    print(f"   ONNX: {onnx_path}")
    print(f"   Best model: {config['model_paths']['best_model']}")
    print(f"   –ú–µ—Ç—Ä–∏–∫–∏: {config['model_paths']['metrics']}")
    
    print(f"\nüìä –û–¢–ß–ï–¢–´:")
    print(f"   –ë–µ–Ω—á–º–∞—Ä–∫: reports/benchmark_results.json")
    if not args.skip_optimization:
        print(f"   –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: reports/optimization_report.json")
    
    print("\n‚úÖ –ü–ê–ô–ü–õ–ê–ô–ù –í–´–ü–û–õ–ù–ï–ù –£–°–ü–ï–®–ù–û!")
    print("=" * 60)

if __name__ == "__main__":
    main()