"""
–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PyTorch –º–æ–¥–µ–ª–∏ –≤ ONNX —Ñ–æ—Ä–º–∞—Ç
"""
import torch
import torch.nn as nn
import onnx
import onnxruntime as ort
import numpy as np
from pathlib import Path
import json
import time
from typing import Dict, Any

from src.utils.logger import model_logger
from src.ml_pipeline.training.train_model import CreditScoringNN

class ModelConverter:
    """–ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä –º–æ–¥–µ–ª–µ–π –≤ ONNX —Ñ–æ—Ä–º–∞—Ç"""
    
    def __init__(self, model_path: str, input_size: int):
        self.model_path = Path(model_path)
        self.input_size = input_size
        
    def convert_to_onnx(self, output_path: str = None, opset_version: int = 13) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ ONNX"""
        if output_path is None:
            output_path = self.model_path.with_suffix('.onnx')
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ PyTorch
        model = CreditScoringNN(input_size=self.input_size)
        model.load_state_dict(torch.load(self.model_path, map_location='cpu'))
        model.eval()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        dummy_input = torch.randn(1, self.input_size)
        
        # –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX
        torch.onnx.export(
            model,
            dummy_input,
            output_path,
            export_params=True,
            opset_version=opset_version,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞
        onnx_model = onnx.load(output_path)
        onnx.checker.check_model(onnx_model)
        
        model_logger.info(f"Model converted to ONNX: {output_path}")
        model_logger.info(f"Input shape: {dummy_input.shape}")
        
        return str(output_path)
    
    def validate_conversion(self, onnx_path: str, num_samples: int = 100) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏"""
        model_logger.info("Validating ONNX conversion...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ PyTorch
        pt_model = CreditScoringNN(input_size=self.input_size)
        pt_model.load_state_dict(torch.load(self.model_path, map_location='cpu'))
        pt_model.eval()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ ONNX –º–æ–¥–µ–ª–∏
        ort_session = ort.InferenceSession(onnx_path)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        test_inputs = torch.randn(num_samples, self.input_size)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è PyTorch
        pt_predictions = []
        with torch.no_grad():
            for i in range(num_samples):
                pt_output = pt_model(test_inputs[i:i+1])
                pt_predictions.append(pt_output.numpy())
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ONNX Runtime
        onnx_predictions = []
        for i in range(num_samples):
            ort_inputs = {ort_session.get_inputs()[0].name: test_inputs[i:i+1].numpy()}
            ort_output = ort_session.run(None, ort_inputs)[0]
            onnx_predictions.append(ort_output)
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        pt_array = np.concatenate(pt_predictions, axis=0)
        onnx_array = np.concatenate(onnx_predictions, axis=0)
        
        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        abs_diff = np.abs(pt_array - onnx_array)
        mae = np.mean(abs_diff)
        max_diff = np.max(abs_diff)
        mse = np.mean((pt_array - onnx_array) ** 2)
        
        validation_results = {
            'mae': float(mae),
            'max_diff': float(max_diff),
            'mse': float(mse),
            'num_samples': num_samples,
            'conversion_valid': mae < 1e-4,  # –ü–æ—Ä–æ–≥ –¥–æ–ø—É—Å—Ç–∏–º–æ–π –æ—à–∏–±–∫–∏
            'pt_shape': pt_array.shape,
            'onnx_shape': onnx_array.shape
        }
        
        model_logger.info(f"Validation results: {validation_results}")
        
        if validation_results['conversion_valid']:
            model_logger.info("‚úÖ ONNX conversion validated successfully!")
        else:
            model_logger.warning("‚ö†Ô∏è ONNX conversion validation failed!")
        
        return validation_results
    
    def benchmark_performance(self, onnx_path: str, pt_model_path: str = None) -> Dict[str, Any]:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ PyTorch –∏ ONNX Runtime"""
        model_logger.info("Benchmarking performance...")
        
        if pt_model_path is None:
            pt_model_path = self.model_path
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
        pt_model = CreditScoringNN(input_size=self.input_size)
        pt_model.load_state_dict(torch.load(pt_model_path, map_location='cpu'))
        pt_model.eval()
        
        ort_session = ort.InferenceSession(onnx_path)
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        batch_sizes = [1, 8, 16, 32, 64, 128]
        num_iterations = 100
        
        benchmark_results = {
            'pytorch': {},
            'onnx': {}
        }
        
        for batch_size in batch_sizes:
            model_logger.info(f"Benchmarking batch size: {batch_size}")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            test_input = torch.randn(batch_size, self.input_size)
            
            # PyTorch benchmark
            pt_times = []
            with torch.no_grad():
                for _ in range(num_iterations):
                    start_time = time.perf_counter()
                    _ = pt_model(test_input)
                    end_time = time.perf_counter()
                    pt_times.append((end_time - start_time) * 1000)  # –º—Å
            
            # ONNX Runtime benchmark
            onnx_times = []
            ort_inputs = {ort_session.get_inputs()[0].name: test_input.numpy()}
            
            for _ in range(num_iterations):
                start_time = time.perf_counter()
                _ = ort_session.run(None, ort_inputs)
                end_time = time.perf_counter()
                onnx_times.append((end_time - start_time) * 1000)  # –º—Å
            
            # –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            benchmark_results['pytorch'][batch_size] = {
                'mean_ms': np.mean(pt_times),
                'std_ms': np.std(pt_times),
                'p95_ms': np.percentile(pt_times, 95),
                'throughput_rps': batch_size / (np.mean(pt_times) / 1000)
            }
            
            benchmark_results['onnx'][batch_size] = {
                'mean_ms': np.mean(onnx_times),
                'std_ms': np.std(onnx_times),
                'p95_ms': np.percentile(onnx_times, 95),
                'throughput_rps': batch_size / (np.mean(onnx_times) / 1000)
            }
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        comparison = {}
        for batch_size in batch_sizes:
            pt_mean = benchmark_results['pytorch'][batch_size]['mean_ms']
            onnx_mean = benchmark_results['onnx'][batch_size]['mean_ms']
            
            comparison[batch_size] = {
                'speedup': pt_mean / max(onnx_mean, 1e-6),
                'pt_throughput': benchmark_results['pytorch'][batch_size]['throughput_rps'],
                'onnx_throughput': benchmark_results['onnx'][batch_size]['throughput_rps']
            }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = {
            'benchmark_results': benchmark_results,
            'comparison': comparison,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'hardware': {
                'cpu': torch.get_num_threads(),
                'device': 'CPU'
            }
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        results_path = Path('reports/onnx_benchmark.json')
        results_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        model_logger.info(f"Benchmark results saved to: {results_path}")
        
        return results

def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Convert PyTorch model to ONNX')
    parser.add_argument('--model_path', type=str, default='models/credit_scoring.pth',
                       help='Path to PyTorch model')
    parser.add_argument('--input_size', type=int, default=20,
                       help='Input size for the model')
    parser.add_argument('--output_path', type=str, default=None,
                       help='Output path for ONNX model')
    
    args = parser.parse_args()
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
    converter = ModelConverter(args.model_path, args.input_size)
    onnx_path = converter.convert_to_onnx(args.output_path)
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    validation_results = converter.validate_conversion(onnx_path)
    
    # –ë–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥
    benchmark_results = converter.benchmark_performance(onnx_path)
    
    print(f"\n{'='*50}")
    print("CONVERSION SUMMARY:")
    print(f"{'='*50}")
    print(f"‚úÖ ONNX model saved: {onnx_path}")
    print(f"‚úÖ Validation MAE: {validation_results['mae']:.6f}")
    print(f"‚úÖ Conversion valid: {validation_results['conversion_valid']}")
    
    # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è batch_size=32
    if 32 in benchmark_results['comparison']:
        comp = benchmark_results['comparison'][32]
        print(f"üìä Performance comparison (batch_size=32):")
        print(f"   PyTorch throughput: {comp['pt_throughput']:.1f} RPS")
        print(f"   ONNX throughput: {comp['onnx_throughput']:.1f} RPS")
        print(f"   Speedup: {comp['speedup']:.2f}x")

if __name__ == "__main__":
    main()